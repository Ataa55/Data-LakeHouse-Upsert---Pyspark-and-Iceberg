{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2fe1621-87a7-456b-a540-540e5edff67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; } </style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c26b9-14fa-48a8-833c-39b14db82cb1",
   "metadata": {},
   "source": [
    "notes\n",
    "if the number of data cols are the same at the source and target but there are diffrent column then it's just column renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84040731-5971-4b20-be36-4ee957a415c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## consts\n",
    "\n",
    "# type_mapping = {\n",
    "#     \"string\": \"STRING\",\n",
    "#     \"integer\": \"INT\",\n",
    "#     \"float\": \"FLOAT\",\n",
    "#     \"double\": \"DOUBLE\",\n",
    "#     \"date\": \"DATE\",\n",
    "#     \"boolean\": \"BOOLEAN\"\n",
    "# }\n",
    "\n",
    "\n",
    "# ## utils\n",
    "# def connect_to_db(postgres_cred):\n",
    "    \n",
    "#     engine = create_engine(f'postgresql://{postgres[\"USER\"]}:{postgres[\"PASSWORD\"]}@{postgres[\"HOST\"]}:{postgres[\"PORT\"]}/{postgres[\"db\"]}')\n",
    "#     try:\n",
    "#         with engine.connect() as conn:             \n",
    "#             print(f\"succefuly connected to {postgres['db']} at host: {postgres['HOST']}\")\n",
    "#         return engine\n",
    "        \n",
    "#     except exc.SQLAlchemyError as e:\n",
    "#         print(f\"connection failed !\")\n",
    "#         print(f\"Error {e}\")\n",
    "\n",
    "\n",
    "# def write_df_to_db(df, table_name, engine):\n",
    "    \n",
    "#     try:\n",
    "#         print(f\"start writing data to {table_name}\")\n",
    "#         start_time  = time.time() \n",
    "#         df.to_sql(name = table_name, con = engine, if_exists='append', index=False, method = \"multi\", chunksize=1000)\n",
    "#         end_time  = time.time() \n",
    "#         print(f\"Data of {len(df)} record Successfully written to the {table_name} table\")\n",
    "#         print(f\"the proccess toke {end_time - start_time} seconds\")\n",
    "    \n",
    "#     except exc.SQLAlchemyError as e:\n",
    "#         print(f\"failed to write data to {table_name} table\")\n",
    "#         print(f\"Error: {e}\")\n",
    "\n",
    "#     except ValueError as ve:\n",
    "#         print(f\"ValueError occured during the write process\")\n",
    "#         print(f\"Error: {ve}\")\n",
    "    \n",
    "#     except Exception as ex:\n",
    "#         print(f\"An expected error occured\")\n",
    "#         print(f\"Error: {ex}\")\n",
    "    \n",
    "\n",
    "# def finalize_things(engine):\n",
    "#     engine.dispose()\n",
    "#     print(\"engine disposed and connection closed successfully\")\n",
    "\n",
    "\n",
    "\n",
    "# def start_spark_session(spark_master, app_name, memory):\n",
    "#     spark = (SparkSession.builder.appName(app_name)\n",
    "#             .config(\"spark.master\",spark_master)\n",
    "#             .config(\"spark.driver.memory\",memory)\n",
    "#             .config(\"spark.executer.memory\",memory)\n",
    "#             .config(\"spark.jars\", \"/opt/spark/jars/postgresql-42.2.18.jar\")\n",
    "#             .getOrCreate())\n",
    "#     return spark\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def check_if_schema_exists(spark, schema_name):\n",
    "#     #check if the provided schema exists or not\n",
    "    \n",
    "#     database_count = spark.sql(f\"show databases like '{schema_name}'\").count()\n",
    "#     if database_count:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "\n",
    "# def check_if_table_exists(spark, schema_name, table_name):\n",
    "#     #first check if the provided schema exists or not, raise exception if not exist\n",
    "#     #then check if the table exists or not, also raise exception if not exists\n",
    "#     if check_if_schema_exists(spark, schema_name):\n",
    "#         spark.sql(f\"USE {schema_name}\")\n",
    "#         table_count = spark.sql(f\"show tables like '{table_name}'\").count()\n",
    "            \n",
    "#         if table_count:\n",
    "#             return True\n",
    "            \n",
    "#         else:\n",
    "#             return False\n",
    "#     else:\n",
    "#         raise Exception(f\"schema: {schema_name} doesn't exist\")\n",
    "        \n",
    "# def get_drift_cols(df_source, df_target):\n",
    "#     #check if any column at the source missed in the target\n",
    "    \n",
    "#     drift_cols = {}   \n",
    "#     # get the source and target fields\n",
    "#     source_fields = df_source.schema.fields\n",
    "#     target_fields = df_target.schema.fields\n",
    "\n",
    "#     # get any missed cols\n",
    "#     missed_fields = set(df_source.schema.fields) - set(df_target.schema.fields)\n",
    "\n",
    "#     # detect and flag the columns that dropped or added to the source \n",
    "#     added_cols_to_the_source = [field for field in source_fields if field not in target_fields ]\n",
    "#     dropped_cols_at_the_source = [field for field in target_fields if field not in source_fields ]\n",
    "    \n",
    "#     if added_cols_to_the_source or dropped_cols_at_the_source:\n",
    "#         for field in added_cols_to_the_source:\n",
    "#             drift_cols[str(field.name)] = [str(field.dataType)[:-6],\"add\"]\n",
    "#         for field in dropped_cols_at_the_source:\n",
    "#             drift_cols[str(field.name)] = [str(field.dataType)[:-6],\"drop\"]\n",
    "    \n",
    "            \n",
    "#         # drift_cols = {col: type_mapping.get(dtype.lower(), dtype.upper()) for col, dtype in drift_cols.items()}\n",
    "#         print(f\"new actions at the source {list(drift_cols.keys())}\")\n",
    "        \n",
    "\n",
    "#     elif len(source_fields) == len(target_fields):\n",
    "#         # flag this column as renamed if the source and target culomns are the same in number but there are changes\n",
    "#         possible_renamed_cols_at_the_source = [field for field in source_fields if field not in target_fields ]\n",
    "\n",
    "#         for field in possible_renamed_cols_at_the_source:\n",
    "#             drift_cols[str(field.name)] = [str(field.dataType)[:-6],\"rename\"]\n",
    "                    \n",
    "#         # drift_cols = {col: type_mapping.get(dtype.lower(), dtype.upper()) for col, dtype in drift_cols.items()}\n",
    "#         print(f\"possible renamed columns at the source {list(drift_cols.keys())}\")\n",
    "               \n",
    "#     else:\n",
    "#         print(\"no changes at the source schema\")\n",
    "#         return None\n",
    "#     return drift_cols\n",
    "\n",
    "# def apply_hash(df_source, df_target, join_on):\n",
    "#     \"\"\" \n",
    "#         this func uses md5 function to apply hash, \n",
    "#         here we will add hash column to the source and target data frames\n",
    "#         returns those two data frames with hash column \n",
    "#     \"\"\"\n",
    "#     data_cols = [col for col in df_target.columns]\n",
    "#     hash_func = F.md5(F.concat_ws(\"|\", *[F.col(col) for col in data_cols]))\n",
    "#     df_source = df_source.withColumn(\"hash_value\", hash_func).alias(\"source\")\n",
    "#     df_target = df_target.withColumn(\"hash_value\", hash_func).alias(\"target\")\n",
    "#     return df_source, df_target\n",
    "    \n",
    "\n",
    "# def get_source_changes(df_source, df_target, join_keys):\n",
    "#     \"\"\"\n",
    "#         this function takes two data frames and return dataframe \n",
    "#         that contains any record changes (update, insert, delete)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # add hash column that saves hased value for the data so we can check an update\n",
    "#     data_cols= [col for col in df_target.columns]\n",
    "#     hashed_source, hashed_target = apply_hash(df_source, df_target, join_keys)\n",
    "\n",
    "#     # apply full outer join on the source and the target by the key column to detect any changes \n",
    "#     join_statment = [hashed_source[join_key] == hashed_target[join_key] for join_key in join_keys]\n",
    "#     join_result = hashed_source.join(hashed_target, join_statment, \"full\")\n",
    "    \n",
    "#     # filter conditions to select update, insert and delete records from the join result\n",
    "#     update_condition = f\"source.hash_value != target.hash_value\"\n",
    "    \n",
    "#     insert_key_condition = \"and \".join([f\"target.{key} is null \" for key in join_keys])\n",
    "#     insert_condition = f\"{insert_key_condition} and target.hash_value is null\"\n",
    "    \n",
    "#     delete_key_condition = \"and \".join([f\"source.{key} is null \" for key in join_keys])\n",
    "#     delete_condition = f\"{delete_key_condition} and source.hash_value is null\"\n",
    "\n",
    "#     ## filter and flag the changes\n",
    "    \n",
    "#     # >> get updtas\n",
    "#     filter_updated_data = join_result.filter(update_condition).select(\"source.*\")\n",
    "#     updated_data = filter_updated_data.withColumn(\"cdc_flag\", F.lit(\"U\"))\n",
    "    \n",
    "#     # >> get inserts                                                     \n",
    "#     filter_inserted_data = join_result.filter(insert_condition).select(\"source.*\")\n",
    "#     inserted_data = filter_inserted_data.withColumn(\"cdc_flag\", F.lit(\"I\"))\n",
    "    \n",
    "#     # >> get deletes\n",
    "#     filter_deleted_data = join_result.filter(delete_condition).select(\"target.*\")\n",
    "#     deleted_data = filter_deleted_data.withColumn(\"cdc_flag\", F.lit(\"D\"))\n",
    "\n",
    "#     # >> union all the changes into one dataframe \n",
    "#     upserted_data = updated_data.unionByName(inserted_data).unionByName(deleted_data).select(data_cols+[\"cdc_flag\"])\n",
    "    \n",
    "#     return upserted_data,join_result\n",
    "\n",
    "# def upsert_target(spark, schema_name, source_table_name, target_table_name, key_columns, upsert_flag ):\n",
    "    \n",
    "#     # check if the schema and tables are exists, raise exception if any not exist\n",
    "#     for table in [source_table_name, target_table_name]:\n",
    "#         if not check_if_table_exists(spark, schema_name, table):\n",
    "#             raise Exception(f\"Table: {table} doesn't exist in the schema: {schema_name}\")\n",
    "            \n",
    "#     # get the sourse and target data    \n",
    "#     df_source = spark.sql(f\"select * from {schema_name}.{source_table_name}\")\n",
    "#     df_target = spark.sql(f\"select * from {schema_name}.{target_table_name}\")\n",
    "    \n",
    "#     # get any new columns at the source and alter the target schema:\n",
    "#     if get_drift_cols(df_source, df_target):\n",
    "#         new_source_fields = get_drift_cols(df_source, df_target)\n",
    "#         columns_to_add = \", \".join([f\"{col} {type_mapping[dtype[0].lower()]}\" \n",
    "#                                     for col, dtype in new_source_fields.items() if dtype[1] == \"add\"])\n",
    "\n",
    "#         columns_to_drop = \", \".join([f\"{col}\" \n",
    "#                                     for col, dtype in new_source_fields.items() if dtype[1] == \"drop\"])\n",
    "\n",
    "#         columns_to_rename = \", \".join([f\"{col} {type_mapping[dtype[0].lower()]}\" \n",
    "#                                     for col, dtype in new_source_fields.items() if dtype[1] == \"rename\"])\n",
    "        \n",
    "#         alter_add_query = f\"alter table {schema_name}.{target_table_name} add columns ({columns_to_add})\"\n",
    "#         alter_drop_query = f\"alter table {schema_name}.{target_table_name} drop columns {columns_to_drop}\"\n",
    "        \n",
    "        \n",
    "#         if columns_to_add:\n",
    "#             spark.sql(alter_add_query)\n",
    "#         if columns_to_drop:\n",
    "#             spark.sql(alter_drop_query)\n",
    "            \n",
    "#         print(new_source_fields)\n",
    "#         #read the new altered target\n",
    "#         df_target = spark.sql(f\"select * from {schema_name}.{target_table_name}\")\n",
    "    \n",
    "    \n",
    "#     # get the delta and changes from the source and save it temporarely\n",
    "    \n",
    "#     changes, join_result= get_source_changes(df_source, df_target, key_columns)\n",
    "#     print(f\"source changes {changes.show()}\")\n",
    "    \n",
    "#     if not changes.count():\n",
    "#         return f\"no changes at source: {source_table_name}\", None\n",
    "#     changes.createOrReplaceTempView(\"changes\")\n",
    "\n",
    "#     # merge the changes (updates, inserts, deletes) into the target\n",
    "#     merge_condition = \" and\".join([f\"{schema_name}.{target_name}.{key_column} = changes.{key_column}\" \n",
    "#                                    for key_column in key_columns])\n",
    "#     try:\n",
    "#         spark.sql(f\"use database {schema_name}\")\n",
    "\n",
    "#         merge_upserts_statment = f\"\"\" MERGE INTO {schema_name}.{target_table_name} \n",
    "#                                       using (select * from changes) \n",
    "#                                       on {merge_condition}\n",
    "#                                       WHEN MATCHED AND changes.{upsert_flag} = 'U' THEN UPDATE SET *\n",
    "#                                       WHEN MATCHED AND changes.{upsert_flag} = 'D' THEN DELETE\n",
    "#                                       WHEN NOT MATCHED THEN INSERT *\n",
    "#                                   \"\"\"\n",
    "#         spark.sql(merge_upserts_statment)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"some error happened while merging{e}\")    \n",
    "#     return changes, join_result\n",
    "\n",
    "\n",
    "def alter_datalake_target(spark, schema_name, source_table_name, target_table_name):\n",
    "# check if the schema and tables are exists, raise exception if any not exist\n",
    "    for table in [source_table_name, target_table_name]:\n",
    "        if not check_if_table_exists(spark, schema_name, table):\n",
    "            raise Exception(f\"Table: {table} doesn't exist in the schema: {schema_name}\")\n",
    "            \n",
    "    # get the sourse and target data    \n",
    "    df_source = spark.sql(f\"select * from {schema_name}.{source_table_name}\")\n",
    "    df_target = spark.sql(f\"select * from {schema_name}.{target_table_name}\")\n",
    "    \n",
    "    # get any new columns at the source and alter the target schema:\n",
    "    new_source_fields = {}\n",
    "    \n",
    "    if get_drift_cols(df_source, df_target):\n",
    "        new_source_fields = get_drift_cols(df_source, df_target)\n",
    "        \n",
    "        columns_to_add = \", \".join([f\"{col} {spark_type_mapping[dtype[0].lower()]}\" \n",
    "                                    for col, dtype in new_source_fields.items() if dtype[1] == \"add\"])\n",
    "    \n",
    "        columns_to_drop = \", \".join([f\"{col}\" \n",
    "                                    for col, dtype in new_source_fields.items() if dtype[1] == \"drop\"])\n",
    "      \n",
    "        columns_to_rename = \", \".join([f\"{col} {spark_type_mapping[dtype[0].lower()]}\" \n",
    "                                    for col, dtype in new_source_fields.items() if dtype[1] == \"rename\"])\n",
    "        \n",
    "        alter_add_query = f\"alter table {schema_name}.{target_table_name} add columns ({columns_to_add})\"\n",
    "        alter_drop_query = f\"alter table {schema_name}.{target_table_name} drop columns {columns_to_drop}\"\n",
    "        \n",
    "        \n",
    "        if columns_to_add:\n",
    "            spark.sql(alter_add_query)\n",
    "        if columns_to_drop:\n",
    "            spark.sql(alter_drop_query)\n",
    "            \n",
    "    return new_source_fields\n",
    "\n",
    "\n",
    "\n",
    "def check_if_db_table_exists(engine, schema_name, table_name):\n",
    "    \n",
    "    inspector = inspect(engine)\n",
    "    existance_flag = inspector.has_table(table_name, schema = schema_name)\n",
    "    \n",
    "    return existance_flag\n",
    "        \n",
    "    \n",
    "\n",
    "def alter_db_target(engine, db_table, schema_name, new_source_fields):\n",
    "    \"\"\" This function hits the database to apply and alter any changes at the tables' schema  \"\"\"\n",
    "\n",
    "    changes_db_table = f\"{db_table}_cdc\"\n",
    "    \n",
    "    columns_to_add = \"\"\n",
    "    columns_to_drop = \"\"\n",
    "    \n",
    "    if new_source_fields:        \n",
    "        \n",
    "        columns_to_add = \",\".join([f\"{col} {postgres_type_mapping[dtype[0].lower()]}\" \n",
    "                                    for col, dtype in new_source_fields.items() if dtype[1] == \"add\"])\n",
    "    \n",
    "        columns_to_drop = \",\".join([f\"{col}\" \n",
    "                                    for col, dtype in new_source_fields.items() if dtype[1] == \"drop\"])\n",
    "      \n",
    "\n",
    "    for table in [db_table, changes_db_table]:\n",
    "        if not check_if_db_table_exists(engine, schema_name, table):\n",
    "           raise Exception(f\"Table: {table} doesn't exist at the schema: {schema_name} at {postgres_cred['db']}\")\n",
    "            \n",
    "    if columns_to_add or columns_to_drop:\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            \n",
    "            if columns_to_add:\n",
    "                columns_to_add = columns_to_add.split(\",\")\n",
    "                print(columns_to_add)\n",
    "                \n",
    "                for col in columns_to_add:\n",
    "                    try:  \n",
    "                        add_query = text(f\"\"\"\n",
    "                        ALTER TABLE {db_table} ADD COLUMN {col}\n",
    "                        \"\"\")\n",
    "                \n",
    "                        conn.execute(add_query)\n",
    "                        conn.commit()\n",
    "                    except Exception as e:\n",
    "                        print(f\"erorr {e} at the database while execute this query {add_query}\")\n",
    "                    \n",
    "                    \n",
    "            if columns_to_drop:\n",
    "                columns_to_drop = columns_to_drop.split(\",\")\n",
    "                print(columns_to_drop)\n",
    "                \n",
    "                for col in columns_to_drop:\n",
    "                    try:\n",
    "                        drop_query = text(f\"\"\"\n",
    "                                    ALTER TABLE {db_table} drop COLUMN \"{col}\"\n",
    "                                    \"\"\")\n",
    "            \n",
    "                        conn.execute(drop_query)\n",
    "                        conn.commit()\n",
    "                    except Exception as e:\n",
    "                        print(f\"erorr {e} at the database while execute this query {drop_query}\")\n",
    "            \n",
    "\n",
    "def upsert_db_target(engine, db_table, schema_name):\n",
    "    \n",
    "    changes_db_table = f\"{db_table}_cdc\"\n",
    "    \n",
    "    inspector = inspect(engine)\n",
    "    \n",
    "\n",
    "    for table in [db_table, changes_db_table]:\n",
    "        if not check_if_db_table_exists(engine, schema_name, table):\n",
    "           raise Exception(f\"Table: {table} doesn't exist at the schema: {schema_name} at {postgres_cred['db']}\")\n",
    "               \n",
    "    # get the the column names of the target table\n",
    "    db_target_columns = \",\".join(f'\\\"{col[\"name\"]}\\\"' for col in \n",
    "                                 inspector.get_columns(db_table, schema = schema_name))\n",
    "\n",
    "    splitted_db_column = db_target_columns.split(\",\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        try:\n",
    "            insert_query = text(f\"\"\"insert into {db_table}\n",
    "                                        select {db_target_columns}\n",
    "                                        from {changes_db_table} \n",
    "                                        where cdc_flag = 'I'\"\"\")\n",
    "            \n",
    "            conn.execute(insert_query)\n",
    "            conn.commit()\n",
    "        except Exception as e_insert:\n",
    "            print(f\"erorr {e_insert} at the database while execute this query {insert_query}\")\n",
    "            \n",
    "            \n",
    "        set_clause = \", \".join([f\"{col} = cdc.{col}\" for col in splitted_db_column\n",
    "                                                                    if col not in key_column])\n",
    "\n",
    "        try:\n",
    "            update_query = text(f\"\"\"update {db_table}  \n",
    "                                    set {set_clause}\n",
    "                                    from {changes_db_table} as cdc \n",
    "                                    where cdc.cdc_flag = 'U' \n",
    "                                    and {db_table}.\"{key_column[0]}\" = cdc.\"{key_column[0]}\"\n",
    "                                    \"\"\")\n",
    "            conn.execute(update_query)\n",
    "            conn.commit()\n",
    "        except Exception as e_update:\n",
    "            print(f\"erorr {e_update} at the database while execute this query {update_query}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            delete_query = text(f\"\"\"delete from {db_table} as target\n",
    "                                    using {changes_db_table} as cdc\n",
    "                                    where target.\"{key_column[0]}\" = cdc.\"{key_column[0]}\"\n",
    "                                    and cdc.cdc_flag = 'D'\n",
    "                                    \"\"\")\n",
    "            conn.execute(delete_query)\n",
    "            conn.commit()\n",
    "        except Exception as e_delete:\n",
    "            print(f\"erorr {e_delete} at the database while execute this query {delete_query}\")\n",
    "            \n",
    "           \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c69773-e51f-4c6c-9bd6-96ee8bd15c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 07:59:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from sqlalchemy import create_engine, exc, text, inspect\n",
    "import config\n",
    "from utils import * \n",
    "import consts \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Glopal variables\n",
    "    postgres_cred = config.POSTGRS_CREDENTIALS\n",
    "    # spark variables\n",
    "    spark_master = \"spark://af27ca1c49e9:7077\"\n",
    "    app_name = \"pyspark_upsert\"\n",
    "    memory = \"2g\"\n",
    "    \n",
    "    # data lake variables\n",
    "    csv_file = \"/home/iceberg/warehouse/Customers.csv\"\n",
    "    schema_name = \"iceberg\"\n",
    "    db_schema_name = \"public\"\n",
    "    source_table = \"patition_source_customers\"\n",
    "    target_table = \"patition_target_customers\"\n",
    "    key_column = [\"CustomerID\"]\n",
    "    upsert_flag = \"cdc_flag\"\n",
    "    db_table = \"costumers\"\n",
    "    changes_table = f\"{db_table}_cdc\"\n",
    "    \n",
    "    \n",
    "    spark = start_spark_session(spark_master = spark_master, \n",
    "                                app_name = app_name, \n",
    "                                memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c691b591-0d13-4f43-8752-ad9c30787ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://44bbf18cdd61:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7512f80b4a00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "266af6dd-2c69-49bd-a5ba-7c54b61c38eb",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no changes at the source schema\n",
      "+---+----------+------+---+-----------------+----------------------+----------+---------------+-----------+--------+\n",
      "|uid|CustomerID|Gender|Age|Annual Income ($)|Spending Score (1-100)|Profession|Work Experience|Family Size|cdc_flag|\n",
      "+---+----------+------+---+-----------------+----------------------+----------+---------------+-----------+--------+\n",
      "| 11|        11|  MAle| 23|            84000|                    94|Healthcare|              1|          3|       U|\n",
      "+---+----------+------+---+-----------------+----------------------+----------+---------------+-----------+--------+\n",
      "\n",
      "start merging\n",
      "succefuly connected to CUSTOMERS_DWH at host: postgres\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from sqlalchemy import create_engine, exc, text, inspect\n",
    "import config\n",
    "from utils import * \n",
    "import consts \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Glopal variables\n",
    "    postgres_cred = config.POSTGRS_CREDENTIALS\n",
    "    # spark variables\n",
    "    spark_master = \"spark://af27ca1c49e9:7077\"\n",
    "    app_name = \"pyspark_upsert\"\n",
    "    memory = \"2g\"\n",
    "    \n",
    "    # data lake variables\n",
    "    csv_file = \"/home/iceberg/warehouse/Customers.csv\"\n",
    "    schema_name = \"iceberg\"\n",
    "    db_schema_name = \"public\"\n",
    "    source_table = \"patition_source_customers\"\n",
    "    target_table = \"patition_target_customers\"\n",
    "    key_column = [\"CustomerID\"]\n",
    "    upsert_flag = \"cdc_flag\"\n",
    "    db_table = \"costumers\"\n",
    "    changes_table = f\"{db_table}_cdc\"\n",
    "    \n",
    "    \n",
    "    spark = start_spark_session(spark_master = spark_master, \n",
    "                                app_name = app_name, \n",
    "                                memory = memory)\n",
    "\n",
    "    \n",
    "    \n",
    "    df = spark.read.csv(csv_file, \n",
    "                        header=True, \n",
    "                        inferSchema=True)\n",
    "    \n",
    "    df_limited = df.limit(15)\n",
    "    \n",
    "    spark.sql(f\"create database if not exists {schema_name}\")\n",
    "    \n",
    "    df_limited.write.format(\"iceberg\").saveAsTable(f\"{schema_name}.{source_table}\",\n",
    "                                                   mode=\"overwrite\")\n",
    "    \n",
    "    # df_limited.write.format(\"iceberg\").saveAsTable(f\"{schema_name}.{target_table}\")\n",
    "\n",
    "    # write_sdf_to_postgres_db(df_limited, \n",
    "    #                      config.POSTGRS_CREDENTIALS, \n",
    "    #                      db_table, \n",
    "    #                      mode = \"overwrite\")\n",
    "    \n",
    "    # df_source = spark.sql(f\"select * from {schema_name}.{source_table}\")\n",
    "    # df_target = spark.sql(f\"select * from {schema_name}.{target_table}\")\n",
    "    # source_changes , join_result= get_source_changes(df_source, df_target, [\"CustomerID\"])\n",
    "\n",
    "\n",
    "    source_new_actions = alter_datalake_target(spark, \n",
    "                                               schema_name, \n",
    "                                               source_table, \n",
    "                                               target_table)\n",
    "\n",
    "    changes, join_result = get_source_changes(spark, \n",
    "                                              schema_name, \n",
    "                                              source_table, \n",
    "                                              target_table, \n",
    "                                              key_column)\n",
    "\n",
    "    changes.show()\n",
    "    \n",
    "    write_sdf_to_postgres_db(changes, \n",
    "                             config.POSTGRS_CREDENTIALS, \n",
    "                             changes_table, \n",
    "                             mode = \"overwrite\")\n",
    "    \n",
    "    upsert_datalake_target(spark, \n",
    "                           schema_name, \n",
    "                           source_table, \n",
    "                           target_table,\n",
    "                           key_column,\n",
    "                           changes, \n",
    "                           upsert_flag)\n",
    "\n",
    "    engine = connect_to_db(postgres_cred)\n",
    "    \n",
    "    alter_db_target(engine, db_table, db_schema_name, source_new_actions)\n",
    "\n",
    "    upsert_db_target(engine, db_table, db_schema_name)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc4fec8f-38c1-4c30-9824-468487fe96d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "|uid|CustomerID|Gender|Age|Annual Income ($)|Spending Score (1-100)|   Profession|Work Experience|Family Size|\n",
      "+---+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "|  1|         1|Female| 25|            15000|                    39|     Engineer|              1|          4|\n",
      "|  2|         2|  Male| 42|            35000|                    81|     Engineer|              3|          3|\n",
      "|  3|         3|  Male| 42|            35000|                    81|     Engineer|              3|          3|\n",
      "|  5|         4|Female| 50|             5300|                    77|   Healthcare|              5|          9|\n",
      "|  6|         5|  Male| 31|             5700|                    40|Entertainment|              2|          6|\n",
      "|  7|         6|  Male| 27|             5700|                    40|       Lawyer|              2|          6|\n",
      "|  0|         7|  Male| 27|             5700|                    40|       Lawyer|              2|          6|\n",
      "| 90|         8|  Male| 35|            31000|                     6|   Healthcare|              1|          3|\n",
      "| 10|         9|Female| 23|            84000|                    94|   Healthcare|              1|          3|\n",
      "| 10|        10|  MAle| 23|            84000|                    94|   Healthcare|              1|          3|\n",
      "| 11|        11|  MAle| 23|            84000|                    94|   Healthcare|              1|          3|\n",
      "+---+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from iceberg.patition_target_customers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfc77edd-9fdb-47bc-b752-795125b46403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "|uid|CustomerID|Gender|Age|Annual Income ($)|Spending Score (1-100)|   Profession|Work Experience|Family Size|\n",
      "+---+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "| 11|        11|  MAle| 23|            84000|                    94|   Healthcare|              1|          3|\n",
      "|  1|         1|Female| 25|            15000|                    39|     Engineer|              1|          4|\n",
      "|  2|         2|  Male| 42|            35000|                    81|     Engineer|              3|          3|\n",
      "|  3|         3|  Male| 42|            35000|                    81|     Engineer|              3|          3|\n",
      "|  5|         4|Female| 50|             5300|                    77|   Healthcare|              5|          9|\n",
      "|  6|         5|  Male| 31|             5700|                    40|Entertainment|              2|          6|\n",
      "|  7|         6|  Male| 27|             5700|                    40|       Lawyer|              2|          6|\n",
      "|  0|         7|  Male| 27|             5700|                    40|       Lawyer|              2|          6|\n",
      "| 90|         8|  Male| 35|            31000|                     6|   Healthcare|              1|          3|\n",
      "| 10|         9|Female| 23|            84000|                    94|   Healthcare|              1|          3|\n",
      "| 10|        10|  MAle| 23|            84000|                    94|   Healthcare|              1|          3|\n",
      "+---+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from iceberg.patition_target_customers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6ace101-5254-424d-b844-aa64800cc16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_limited.write.format(\"iceberg\").saveAsTable(f\"{schema_name}.{target_table}\")\n",
    "\n",
    "write_sdf_to_postgres_db(df_limited, \n",
    "                     config.POSTGRS_CREDENTIALS, \n",
    "                     db_table, \n",
    "                     mode = \"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e7506d-bd23-4e80-b5b7-5269f3e2e1c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TEMP_TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create the temporary view `spark_view` because it already exists.\nChoose a different name, drop or replace the existing view,  or add the IF NOT EXISTS clause to tolerate pre-existing views.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcreate global temp view spark_view as select * from iceberg.patition_target_customers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TEMP_TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create the temporary view `spark_view` because it already exists.\nChoose a different name, drop or replace the existing view,  or add the IF NOT EXISTS clause to tolerate pre-existing views."
     ]
    }
   ],
   "source": [
    "spark.sql(\"create global temp view spark_view as select * from iceberg.patition_target_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c06daaa-bd1a-499b-8ca2-d334f2732781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `global_temp`.`spark_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [global_temp, spark_view], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect * from global_temp.spark_view\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `global_temp`.`spark_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [global_temp, spark_view], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from global_temp.spark_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e6cc440-0df6-4d51-b2ac-6a071631ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8c85ea5-850c-41c3-8066-c0063c0e7f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2024-12-01 11:01:...|5870829690807936430|               NULL|               true|\n",
      "|2024-12-01 11:08:...|3606740596860261162|5870829690807936430|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from iceberg.patition_target_customers.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cfc05883-2b19-44a4-8f30-089026713a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+---------+----------+----------+\n",
      "|uid|CustomerID|Gender|Age|Annual Income ($)|Spending Score (1-100)|   Profession|Work Experience|Family Size|my_column|now_column|      Date|\n",
      "+---+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+---------+----------+----------+\n",
      "|  1|         1|Female| 25|            15000|                    39|     Engineer|              1|          4|     ataa|kjbhvsdbfk|2024-03-07|\n",
      "|  2|         2|  Male| 42|            35000|                    81|     Engineer|              3|          3|        2|      NULL|2024-03-31|\n",
      "|  3|         3|  Male| 42|            35000|                    81|     Engineer|              3|          3|     haab|      NULL|2024-03-06|\n",
      "|  5|         4|Female| 50|             5300|                    77|   Healthcare|              5|          9|      zoo|      NULL|2024-03-11|\n",
      "|  6|         5|  Male| 31|             5700|                    40|Entertainment|              2|          6|     NULL|      NULL|2024-03-12|\n",
      "| 70|         6|  Male| 27|             5700|                    40|       Lawyer|              2|          6|     NULL|      NULL|2024-02-17|\n",
      "| 80|         7|  Male| 27|             5700|                    40|       Lawyer|              2|          6|     NULL|      NULL|2024-03-20|\n",
      "| 90|         8|Female| 35|            31000|                     6|   Healthcare|              1|          3|     NULL|      NULL|2024-03-31|\n",
      "|100|         9|Female| 23|            84000|                    94|   Healthcare|              1|          3|     NULL|      NULL|2024-02-06|\n",
      "| 10|        10|  male| 23|            84000|                    94|   Healthcare|              1|          3|     NULL|      NULL|2024-02-02|\n",
      "| 11|        11|Female| 64|            97000|                     3|     Engineer|              0|          3|     NULL|      NULL|2024-02-21|\n",
      "| 14|        12|  Male| 67|             7000|                    14|     Engineer|              1|          3|     NULL|      NULL|2024-03-07|\n",
      "+---+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+---------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import random \n",
    "from pyspark.sql.types import StringType, FloatType, DateType\n",
    "\n",
    "def generate_random_date(start_date, end_date):\n",
    "    # Convert start_date and end_date to `yyyy-MM-dd` format\n",
    "    start_date_lit = F.lit(start_date)\n",
    "    end_date_lit = F.lit(end_date)\n",
    "    \n",
    "    # Calculate the random date\n",
    "    random_date = F.expr(f\"date_add('{start_date}', cast(rand() * datediff('{end_date}', '{start_date}') as int))\")\n",
    "    return random_date\n",
    "    \n",
    "    \n",
    "start_date = '2024-02-01'\n",
    "end_date = '2024-04-01'\n",
    "\n",
    "# Add a random date column\n",
    "df_with_random_dates = df.withColumn(\"Date\", generate_random_date(start_date, end_date))\n",
    "\n",
    "                            \n",
    "                    \n",
    "df_with_random_dates.partitionBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba49b30b-4cbf-490d-bdb4-b30fe4c8b129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succefuly connected to CUSTOMERS_DWH at host: postgres\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 08:20:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from sqlalchemy import create_engine, exc, text, inspect\n",
    "import config\n",
    "from utils import * \n",
    "import consts \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Glopal variables\n",
    "    postgres_cred = config.POSTGRS_CREDENTIALS\n",
    "    # spark variables\n",
    "    spark_master = \"spark://af27ca1c49e9:7077\"\n",
    "    app_name = \"pyspark_upsert\"\n",
    "    memory = \"2g\"\n",
    "    \n",
    "    # data lake variables\n",
    "    csv_file = \"/home/iceberg/warehouse/customers_stage.csv\"\n",
    "    schema_name = \"iceberg\"\n",
    "    source_table = \"partition_source_customers\"\n",
    "    target_table = \"partition_target_customers\"\n",
    "    key_column = [\"CustomerID\"]\n",
    "    upsert_flag = \"cdc_flag\"\n",
    "    db_table = \"costumers\"\n",
    "    changes_table = f\"{db_table}_table_cdc\"\n",
    "    \n",
    "    \n",
    "    spark = start_spark_session(spark_master = spark_master, \n",
    "                                app_name = app_name, memory = memory)\n",
    "    \n",
    "    # df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    # df_limited = df.limit(13)\n",
    "    \n",
    "    # spark.sql(f\"create database if not exists {schema_name}\")\n",
    "    # df.write.partitionBy(\"Age\").format(\"iceberg\").saveAsTable(f\"{schema_name}.{source_table}\",\n",
    "                                                   # mode=\"overwrite\")\n",
    "    # df.write.partitionBy(\"new_column\").format(\"iceberg\").saveAsTable(f\"{schema_name}.{source_table}\",\n",
    "    #                                                mode=\"overwrite\")\n",
    "\n",
    "    # query = f\"\"\"select count(*) \n",
    "    #             from\n",
    "    #             information_schema.tables \n",
    "    #             where table_schema = {schema_name} \n",
    "    #                 and table_name = {table_name} \"\"\"\n",
    "    \n",
    "    engine = connect_to_db(postgres_cred)\n",
    "    inspector = inspect(engine)\n",
    "    print(inspector.has_table(\"costumers_table_cdc\", schema = \"public\"))\n",
    "    # with engine.connect() as conn:\n",
    "    #     conn.execute()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24649aae-63e4-4eb3-adb8-b2b7c1b9e9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"alter table iceberg.partition_source_customers drop column Gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91dd4f45-a9fd-4581-aaf6-00283a437200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succefuly connected to CUSTOMERS_DWH at host: postgres\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, exc, text, inspect\n",
    "\n",
    "engine = connect_to_db(postgres_cred)\n",
    "inspector = inspect(engine)\n",
    "print(inspector.has_table(\"costumers_table_cdc\", schema = \"public\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b3bc59d-c2a6-412f-a7cf-9437287ece5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succefuly connected to CUSTOMERS_DWH at host: postgres\n",
      "True\n",
      "[{'name': 'uid', 'type': INTEGER(), 'nullable': True, 'default': None, 'autoincrement': False, 'comment': None}, {'name': 'CustomerID', 'type': INTEGER(), 'nullable': True, 'default': None, 'autoincrement': False, 'comment': None}, {'name': 'Gender', 'type': TEXT(), 'nullable': True, 'default': None, 'autoincrement': False, 'comment': None}, {'name': 'Age', 'type': INTEGER(), 'nullable': True, 'default': None, 'autoincrement': False, 'comment': None}, {'name': 'Annual Income ($)', 'type': INTEGER(), 'nullable': True, 'default': None, 'autoincrement': False, 'comment': None}, {'name': 'Spending Score (1-100)', 'type': INTEGER(), 'nullable': True, 'default': None, 'autoincrement': False, 'comment': None}, {'name': 'Profession', 'type': TEXT(), 'nullable': True, 'default': None, 'autoincrement': False, 'comment': None}, {'name': 'Work Experience', 'type': INTEGER(), 'nullable': True, 'default': None, 'autoincrement': False, 'comment': None}, {'name': 'Family Size', 'type': INTEGER(), 'nullable': True, 'default': None, 'autoincrement': False, 'comment': None}, {'name': 'my_column', 'type': TEXT(), 'nullable': True, 'default': None, 'autoincrement': False, 'comment': None}, {'name': 'now_column', 'type': TEXT(), 'nullable': True, 'default': None, 'autoincrement': False, 'comment': None}, {'name': 'cdc_flag', 'type': TEXT(), 'nullable': False, 'default': None, 'autoincrement': False, 'comment': None}]\n"
     ]
    }
   ],
   "source": [
    "engine = connect_to_db(postgres_cred)\n",
    "inspector = inspect(engine)\n",
    "print(inspector.has_table(\"costumers_table_cdc\", schema = \"public\"))\n",
    "print(inspector.get_columns(\"costumers\", schema = \"public\"))\n",
    "db_columns = \",\".join(col[\"name\"] for col in inspector.get_columns(\"costumers\", schema = \"public\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e78d60e1-86be-4ef4-83a9-10ac38146ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uid',\n",
       " 'CustomerID',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Annual Income ($)',\n",
       " 'Spending Score (1-100)',\n",
       " 'Profession',\n",
       " 'Work Experience',\n",
       " 'Family Size',\n",
       " 'my_column',\n",
       " 'now_column',\n",
       " 'cdc_flag']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_columns.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "215c012a-d044-419f-afe6-b8aac201fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes.write.format(\"jdbc\")\\\n",
    "        .option(\"url\", f\"jdbc:postgresql://postgres:{config.POSTGRS_CREDENTIALS['PORT']}/{config.POSTGRS_CREDENTIALS['db']}\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .option(\"dbtable\", \"test_table\") \\\n",
    "        .option(\"user\", config.POSTGRS_CREDENTIALS[\"USER\"]) \\\n",
    "        .option(\"password\", config.POSTGRS_CREDENTIALS[\"PASSWORD\"]) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ad79b3-e5e7-427e-8cd5-699756d0b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.10.tar.gz (385 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.7/385.7 kB\u001b[0m \u001b[31m427.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: psycopg2\n",
      "  Building wheel for psycopg2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psycopg2: filename=psycopg2-2.9.10-cp39-cp39-linux_x86_64.whl size=472659 sha256=62a87383d67dfab5f75ed939333af07abf56b34832683bc9e3008bb81b45df6c\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/f0/13/36dd45ba7a971c79ded4f3003e5f4652d262195d0e8ea8f249\n",
      "Successfully built psycopg2\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b52a30e6-43ae-4d29-a39f-dab87c8ccb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+---------+--------+\n",
      "|uid|CustomerID|Gender|Age|Annual Income ($)|Spending Score (1-100)|   Profession|Work Experience|Family Size|my_column|cdc_flag|\n",
      "+---+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+---------+--------+\n",
      "|  3|         3|  Male| 42|            35000|                    81|     Engineer|              3|          3|     haab|       U|\n",
      "|  5|         4|Female| 50|             5300|                    77|   Healthcare|              5|          9|      zoo|       U|\n",
      "|  6|         5|  Male| 31|             5700|                    40|Entertainment|              2|          6|     NULL|       U|\n",
      "| 70|         6|  Male| 27|             5700|                    40|       Lawyer|              2|          6|     NULL|       U|\n",
      "| 80|         7|  Male| 27|             5700|                    40|       Lawyer|              2|          6|     NULL|       U|\n",
      "| 90|         8|Female| 35|            31000|                     6|   Healthcare|              1|          3|     NULL|       U|\n",
      "|100|         9|Female| 23|            84000|                    94|   Healthcare|              1|          3|     NULL|       U|\n",
      "| 11|        10|  male| 23|            84000|                    94|   Healthcare|              1|          3|     NULL|       U|\n",
      "| 12|        11|Female| 64|            97000|                     3|     Engineer|              0|          3|     NULL|       U|\n",
      "| 13|        12|  Male| 67|             7000|                    14|     Engineer|              1|          3|     NULL|       U|\n",
      "+---+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "changes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec4a8ac-0ca8-4b2e-8208-61ae33cf2630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2024-11-27 11:23:...| 842133445075260869|               NULL|               true|\n",
      "|2024-11-27 11:45:...|5495001484026353230| 842133445075260869|               true|\n",
      "|2024-11-27 11:46:...|3331717049093625722|5495001484026353230|               true|\n",
      "|2024-11-27 11:46:...|1838334206140450778|3331717049093625722|               true|\n",
      "|2024-11-27 12:03:...|3075584120797630664|1838334206140450778|               true|\n",
      "|2024-11-27 12:05:...|6107534639912780166|3075584120797630664|               true|\n",
      "|2024-11-27 12:08:...|3057032334845989142|6107534639912780166|               true|\n",
      "|2024-11-27 12:09:...|6440120487727355684|3057032334845989142|               true|\n",
      "|2024-11-27 12:12:...|2131501795121033187|6440120487727355684|               true|\n",
      "|2024-11-27 12:17:...|3815639585568450895|2131501795121033187|               true|\n",
      "|2024-11-27 12:17:...|2880779604255117755|3815639585568450895|               true|\n",
      "|2024-11-27 12:18:...|5875028925904351790|2880779604255117755|               true|\n",
      "|2024-11-27 16:13:...|9012525566198417205|5875028925904351790|               true|\n",
      "|2024-11-27 16:41:...|5776329571494013396|9012525566198417205|               true|\n",
      "|2024-11-27 16:48:...|4603189299173988312|5776329571494013396|               true|\n",
      "|2024-11-27 16:49:...| 523380484991932790|4603189299173988312|               true|\n",
      "|2024-11-27 17:06:...| 977717561622280734| 523380484991932790|               true|\n",
      "|2024-11-27 17:07:...|5980331219226369433| 977717561622280734|               true|\n",
      "|2024-11-27 17:09:...|5415803791936511728|5980331219226369433|               true|\n",
      "|2024-11-27 17:10:...|2381800314418478148|5415803791936511728|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from iceberg.target_customers.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b252386-2596-406b-8d0f-e44b0e3fb7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succefuly connected to CUSTOMERS_DWH at host: postgres\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Engine(postgresql://root:***@postgres:5432/CUSTOMERS_DWH)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connect_to_db(postgres_cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d31a204d-5bd1-4ba9-a9ed-573b501c28b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cdc_costumers_table'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changes_table[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f14a75-b418-43d6-a8c7-132a6d922d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(uid=1, CustomerID=1, Gender='Female', Age=25, Annual Income ($)=15000, Spending Score (1-100)=39, Profession='Engineer', Work Experience=1, Family Size=4, my_column=' ataa'),\n",
       " Row(uid=2, CustomerID=2, Gender='Male', Age=42, Annual Income ($)=35000, Spending Score (1-100)=81, Profession='Engineer', Work Experience=3, Family Size=3, my_column=' 2'),\n",
       " Row(uid=3, CustomerID=3, Gender='Male', Age=42, Annual Income ($)=35000, Spending Score (1-100)=81, Profession='Engineer', Work Experience=3, Family Size=3, my_column=' haab'),\n",
       " Row(uid=4, CustomerID=4, Gender='Female', Age=50, Annual Income ($)=5300, Spending Score (1-100)=77, Profession='Healthcare', Work Experience=5, Family Size=9, my_column=' zoo'),\n",
       " Row(uid=60, CustomerID=5, Gender='Female', Age=31, Annual Income ($)=5700, Spending Score (1-100)=40, Profession='Entertainment', Work Experience=2, Family Size=6, my_column=None),\n",
       " Row(uid=70, CustomerID=6, Gender='Male', Age=27, Annual Income ($)=5700, Spending Score (1-100)=40, Profession='Lawyer', Work Experience=2, Family Size=6, my_column=None),\n",
       " Row(uid=80, CustomerID=7, Gender='Male', Age=27, Annual Income ($)=5700, Spending Score (1-100)=40, Profession='Lawyer', Work Experience=2, Family Size=6, my_column=None),\n",
       " Row(uid=90, CustomerID=8, Gender='Female', Age=35, Annual Income ($)=31000, Spending Score (1-100)=6, Profession='Healthcare', Work Experience=1, Family Size=3, my_column=None),\n",
       " Row(uid=1000, CustomerID=9, Gender='Female', Age=23, Annual Income ($)=84000, Spending Score (1-100)=94, Profession='Healthcare', Work Experience=1, Family Size=3, my_column=None),\n",
       " Row(uid=110, CustomerID=10, Gender='male', Age=23, Annual Income ($)=84000, Spending Score (1-100)=94, Profession='Healthcare', Work Experience=1, Family Size=3, my_column=None),\n",
       " Row(uid=120, CustomerID=11, Gender='Female', Age=64, Annual Income ($)=97000, Spending Score (1-100)=3, Profession='Engineer', Work Experience=0, Family Size=3, my_column=None),\n",
       " Row(uid=130, CustomerID=12, Gender='Male', Age=67, Annual Income ($)=7000, Spending Score (1-100)=14, Profession='Engineer', Work Experience=1, Family Size=3, my_column=None),\n",
       " Row(uid=140, CustomerID=13, Gender='Female', Age=35, Annual Income ($)=93000, Spending Score (1-100)=99, Profession='Healthcare', Work Experience=4, Family Size=4, my_column=None)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from iceberg.target_customers order by CustomerID\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "277690a0-eeee-48fe-8c64-5006e0dc6379",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source = spark.sql(\"select * from iceberg.source_customers\")\n",
    "df_target = spark.sql(\"select * from iceberg.target_customers\")\n",
    "missed_fields = set(df_source.schema.fields) - set(df_target.schema.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38df51af-1822-4c39-a0d8-97ac705376f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_target.schema.fields) == len(df_source.schema.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "649caa6f-c71c-4632-b277-a7a83b188865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField('CustomerID', IntegerType(), True),\n",
       " StructField('Gender', StringType(), True),\n",
       " StructField('Age', IntegerType(), True),\n",
       " StructField('Annual Income ($)', IntegerType(), True),\n",
       " StructField('Spending Score (1-100)', IntegerType(), True),\n",
       " StructField('Profession', StringType(), True),\n",
       " StructField('Work Experience', IntegerType(), True),\n",
       " StructField('Family Size', IntegerType(), True)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_source.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dea689bd-4722-4a94-ab2d-451bdfdc3591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(list(set(df_source.schema.fields) - set(df_target.schema.fields)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "609618d8-fdf3-4035-939b-172371cfa63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = start_spark_session(spark_master = \"spark://af27ca1c49e9:7077\", \n",
    "                            app_name = \"pyspark_upsert\", memory = \"2g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82c6422e-ff00-4676-aea7-be537e0a8b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/home/iceberg/warehouse/Customers.csv\", header=True, inferSchema=True)\n",
    "df = df.limit(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ac8e75c-c189-465d-bd2a-0d337b44de7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database if not exists iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61948a55-4924-4d3e-8486-81f02506481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write our source table\n",
    "df.write.saveAsTable(\"iceberg.source_customers\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a03c6652-fb67-46bd-aa21-eb45bd6720db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.saveAsTable(\"iceberg.target_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70bf5694-53cd-4d3c-94e2-095d3d32008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_source = spark.sql(\"select * from iceberg.source_customers\")\n",
    "df_target = spark.sql(\"select * from iceberg.target_customers\")\n",
    "# source_changes = get_source_changes(df_source, df_target, [\"CustomerID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7ef1ca7-4922-42d4-a00a-c730b2204f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+---------+\n",
      "| uid|CustomerID|Gender|Age|Annual Income ($)|Spending Score (1-100)|   Profession|Work Experience|Family Size|my_column|\n",
      "+----+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+---------+\n",
      "|   1|         1|Female| 25|            15000|                    39|     Engineer|              1|          4|     ataa|\n",
      "|   2|         2|  Male| 42|            35000|                    81|     Engineer|              3|          3|        2|\n",
      "|   3|         3|  Male| 42|            35000|                    81|     Engineer|              3|          3|     haab|\n",
      "|   4|         4|Female| 50|             5300|                    77|   Healthcare|              5|          9|      zoo|\n",
      "|  60|         5|Female| 31|             5700|                    40|Entertainment|              2|          6|     NULL|\n",
      "|  70|         6|  Male| 27|             5700|                    40|       Lawyer|              2|          6|     NULL|\n",
      "|  80|         7|  Male| 27|             5700|                    40|       Lawyer|              2|          6|     NULL|\n",
      "|  90|         8|Female| 35|            31000|                     6|   Healthcare|              1|          3|     NULL|\n",
      "|1000|         9|Female| 23|            84000|                    94|   Healthcare|              1|          3|     NULL|\n",
      "| 110|        10|  male| 23|            84000|                    94|   Healthcare|              1|          3|     NULL|\n",
      "|1233|        11|Female| 64|            97000|                     3|     Engineer|              0|          3|     NULL|\n",
      "| 130|        12|  Male| 67|             7000|                    14|     Engineer|              1|          3|     NULL|\n",
      "+----+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from iceberg.target_customers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2669d37b-c9ab-4550-b0d6-323579b2f0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-----------------+----------------------+----------+---------------+-----------+---------+--------+\n",
      "|CustomerID|Gender|Age|Annual Income ($)|Spending Score (1-100)|Profession|Work Experience|Family Size| myColumn|cdc_flag|\n",
      "+----------+------+---+-----------------+----------------------+----------+---------------+-----------+---------+--------+\n",
      "|         1|  Male| 25|            15000|                    39|    Doctor|              1|          4|     ataa|       U|\n",
      "|         2|  Male| 42|            35000|                    81|  Engineer|              3|          3|        2|       U|\n",
      "|         1|  Male| 25|            15000|                    39|    Doctor|              1|          4|     ataa|       U|\n",
      "|         1|  Male| 25|            15000|                    39|    Doctor|              1|          4|     ataa|       U|\n",
      "|         2|  Male| 42|            35000|                    81|  Engineer|              3|          3|        2|       U|\n",
      "|         6|  Male| 27|             5700|                    40|    Lawyer|              2|          6|     NULL|       D|\n",
      "|         6|  Male| 27|             5700|                    40|    Lawyer|              2|          6|     NULL|       D|\n",
      "|         6|  Male| 27|             5700|                    40|    Lawyer|              2|          6|     NULL|       D|\n",
      "+----------+------+---+-----------------+----------------------+----------+---------------+-----------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_changes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "829c4282-5e10-4b46-98fb-e98cb6bbb99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "    ALTER TABLE {schema_name}.{target_name} SET TBLPROPERTIES (\n",
    "  'write.spark.accept-any-schema'='true'\n",
    ")\n",
    "\"\"\"\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2541ba07-a64d-45d6-9984-fabedc9ae40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "|CustomerID|Gender|Age|Annual Income ($)|Spending Score (1-100)|   Profession|Work Experience|Family Size|\n",
      "+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "|         1|  Male| 25|            15000|                    39|     Engineer|              1|          4|\n",
      "|         2|  Male| 42|            35000|                    81|     Engineer|              3|          3|\n",
      "|         3|  Male| 42|            35000|                    81|     Engineer|              3|          3|\n",
      "|         4|Female| 50|             5300|                    77|   Healthcare|              5|          9|\n",
      "|         5|Female| 31|             5700|                    40|Entertainment|              2|          6|\n",
      "|         6|  Male| 22|            58000|                    76|   Healthcare|              0|          2|\n",
      "|         7|Female| 35|            31000|                     6|   Healthcare|              1|          3|\n",
      "|         8|Female| 23|            84000|                    94|   Healthcare|              1|          3|\n",
      "|         9|Female| 23|            84000|                    94|   Healthcare|              1|          3|\n",
      "|        10|Female| 64|            97000|                     3|     Engineer|              0|          3|\n",
      "+----------+------+---+-----------------+----------------------+-------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from iceberg.target_customers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8407a235-9839-4593-a01f-2f053d49c546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema_name = \"iceberg\"\n",
    "source_name = \"source_customers\"\n",
    "target_name = \"target_customers\"\n",
    "key_column = [\"CustomerID\"]\n",
    "upsert_flag = \"cdc_flag\"\n",
    "upsert_target(spark,schema_name, source_name, target_name, key_column, upsert_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ed8c0-71be-4c18-af01-b4e515e9ce4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
